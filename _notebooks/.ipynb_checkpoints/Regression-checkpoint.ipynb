{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Linear Regression Implementation in Python \n",
    "\n",
    "This is a sequel to my [previous blog post](http://theideasmith.github.io/2016/12/19/An-Introduction-To-The-Theory-Of-Numerical-Optimization.html) where I discussed the fundamental theory of linear regression with gradient descent. \n",
    "\n",
    "Here we'll continue our study of gradient descent by implementing it in python. \n",
    "\n",
    "To start, I'll import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Recal that in simple one dimensional regression problems we are given a set of $x,y$ pairs from some unknown function $F(x)$ and are trying to fit some model $f(x,w)$ to best approximate the function $F$ generating the dataset, where $x$ is some $x_i$ from the set of input data and $w$ is a parameter vector we are trying to optimize. \n",
    "\n",
    "For any actual $x_i$ in the input dataset, $\\hat{y} = f(x_i, w)$ ought to be as close as possible to $F(x)=y_i$. \n",
    "\n",
    "The code for implementing regression with gradient descent is actually remarkable simple. \n",
    "We'll start by writing an abstract regression class and then subclass it for our specific choice of model and error function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AbstractRegression:\n",
    "    def __init__(self, xs, ys, w,alpha):\n",
    "        self.w = w\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.a = alpha\n",
    "        self.N = float(len(xs)) # The size of the dataset\n",
    "    \n",
    "    \n",
    "    def f(x,w): return\n",
    "    def r(yhat, ys, xs): return\n",
    "   \n",
    "    \n",
    "    def update(self):        \n",
    "        # Updating weights with respect to error\n",
    "        self.w -= self.a*self.r()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The init method takes `xs`, the set of input $x_i$ input points, `ys` the set of $y=F(x_i)$, `alpha` the learning rate $\\alpha$, `w` an initial parameter vector,\n",
    "\n",
    "In addition, all subclasses must implement three functions: \n",
    "\n",
    "- `f` a function of the form `f(x, w)` where `x` is some arbitrary $x$ value, and `w` is a parameter vector. \n",
    "- `r` a function of the which returns a vector the same length as `w` where $$r_i = \\frac{dE}{dw_i}$$\n",
    "- `E` the actual error function, as in absolute distance, or mean squared error, etc. Note that `E` is not used in the training but is useful to have handy in the class. \n",
    "\n",
    "Now we shall subclass `AbstractRegression` using mean squared error as our error metric and a linear model \n",
    "$$f(x, w) = w_1x+w_2$$\n",
    "\n",
    "In my [previous post](http://theideasmith.github.io/2016/12/19/An-Introduction-To-The-Theory-Of-Numerical-Optimization.html) we derived the learning rates $r$ for such a linear model:\n",
    "\n",
    "$$r_1 = \\frac{1}{N}\\sum^N_j{2(\\hat{y}_j - y_j)x_j}$$\n",
    "$$r_2 = \\frac{1}{N}\\sum^N_j{2(\\hat{y}_j - y_j)}$$\n",
    "\n",
    "and update rule \n",
    "\n",
    "$$w_{i, t+1} = w_{i,t} - \\alpha r_i$$\n",
    "\n",
    "With this settled, see how incredible simple it is to implement this powerful tool:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegression(AbstractRegression):\n",
    "    \n",
    "    def f(self, x,w): \n",
    "        return w[0]*x + w[1]\n",
    "    \n",
    "    def r(self):\n",
    "        yhats = self.f(self.xs,self.w) \n",
    "        loss = yhats - self.ys\n",
    "        return np.array([\n",
    "              (2./self.N)*np.sum(loss*xs),\n",
    "              (2./self.N)*np.sum(loss)  \n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 500.\n",
    "m = 10.\n",
    "b = 10.\n",
    "\n",
    "ws = np.array([100.,100.])\n",
    "xs = np.arange(0,N)\n",
    "ys = m*xs + b + np.random.randn(N)*200\n",
    "\n",
    "regressor = LinearRegression(\n",
    "    xs, ys, ws,\n",
    "    0.0000001)\n",
    "\n",
    "for i in range(10000):\n",
    "    regressor.update()\n",
    "\n",
    "w= regressor.w \n",
    "print w\n",
    "yhat = xs * w[0] + w[1]\n",
    "actual = scatter(xs, ys)\n",
    "predicted = plot(xs, yhat, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
